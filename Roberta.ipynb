{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNNnkh5MAARoYOlmecvnyNO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZahraShourmeij/Roberta/blob/Transformers-articles/Roberta.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import GPT2Tokenizer, RobertaTokenizer, RobertaForMaskedLM, AdamW\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# Load the GPT-2 tokenizer\n",
        "tokenizer_gpt2 = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Read the training dataset\n",
        "try:\n",
        "    df_train = pd.read_csv(\"EXIST2021_training.tsv\", sep=\"\\t\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Training dataset file not found.\")\n",
        "    exit()\n",
        "\n",
        "# Extract labels from the \"task1\" column in the training dataset\n",
        "labels_train = df_train[\"task1\"].tolist()\n",
        "\n",
        "# Tokenize and preprocess each text sample in the training dataset\n",
        "preprocessed_texts_train = []\n",
        "for text_train in df_train[\"text\"]:\n",
        "    # Tokenize the text\n",
        "    tokens_train = tokenizer_gpt2.encode(text_train)\n",
        "    # Extract token IDs from each token\n",
        "    token_ids_train = [token_id_train for token_id_train in tokens_train]\n",
        "    # Append preprocessed token IDs to the preprocessed texts list\n",
        "    preprocessed_texts_train.append(token_ids_train)\n",
        "\n",
        "# Save the preprocessed training dataset to a file\n",
        "with open(\"preprocessed_train_dataset.json\", \"w\") as file_train:\n",
        "    json.dump(preprocessed_texts_train, file_train)\n",
        "\n",
        "# Load RoBERTa tokenizer\n",
        "tokenizer_roberta = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "# Load preprocessed token IDs from the JSON file\n",
        "try:\n",
        "    with open(\"preprocessed_train_dataset.json\", \"r\") as file_train:\n",
        "        preprocessed_texts_train = json.load(file_train)\n",
        "except FileNotFoundError:\n",
        "    print(\"Preprocessed training dataset file not found.\")\n",
        "    exit()\n",
        "#The Transformer-based architectures, such as BERT (Bidirectional Encoder Representations from Transformers) and RoBERTa (Robustly optimized BERT approach), are commonly trained using MLM as one of the pre-training tasks.\n",
        "# Pad or truncate sequences to a fixed length\n",
        "max_length = 64  # adjust as needed\n",
        "input_ids_train = [tokens_train[:max_length] + [tokenizer_roberta.pad_token_id] * (max_length - len(tokens_train)) for tokens_train in preprocessed_texts_train]\n",
        "\n",
        "# Convert token IDs into tensors\n",
        "input_ids_train = torch.tensor(input_ids_train)\n",
        "\n",
        "# Create a TensorDataset\n",
        "dataset_train = TensorDataset(input_ids_train)\n",
        "\n",
        "# Define batch size and create DataLoader for the training dataset\n",
        "#DataLoader sets to shuffle the data before each epoch.\n",
        "batch_size_train = 32\n",
        "dataloader_train = DataLoader(dataset_train, batch_size=batch_size_train, shuffle=True)\n",
        "\n",
        "# Load pre-trained RoBERTa model\n",
        "roberta_model = RobertaForMaskedLM.from_pretrained('roberta-base')\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = AdamW(roberta_model.parameters(), lr=5e-5)\n",
        "\n",
        "# Define training loop\n",
        "num_epochs = 3\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "roberta_model.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    roberta_model.train()\n",
        "    for batch_train in dataloader_train:\n",
        "        # Move batch to device\n",
        "        batch_train = tuple(t.to(device) for t in batch_train)\n",
        "        inputs_train = {\"input_ids\": batch_train[0], \"labels\": batch_train[0]}  # Masked language modeling, predicting the same as input\n",
        "        optimizer.zero_grad()\n",
        "        outputs_train = roberta_model(**inputs_train)\n",
        "        loss_train = outputs_train.loss\n",
        "        loss_train.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Save the trained model\n",
        "roberta_model.save_pretrained(\"roberta-trained-model\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GeveKVo5JvOl",
        "outputId": "91174d4e-f1e4-4753-8b6a-49b199298b81"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the test dataset\n",
        "try:\n",
        "    test_df = pd.read_csv(\"EXIST2021_test_labeled.tsv\", sep=\"\\t\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Test dataset file not found.\")\n",
        "    exit()\n",
        "\n",
        "# Tokenize and preprocess each text sample in the test dataset\n",
        "preprocessed_test_texts = []\n",
        "for text_test in test_df[\"text\"]:\n",
        "    tokens_test = tokenizer_roberta.encode(text_test, max_length=max_length, truncation=True)\n",
        "    preprocessed_test_texts.append(tokens_test)\n",
        "\n",
        "# Save the preprocessed test dataset to a file\n",
        "with open(\"preprocessed_test_dataset.json\", \"w\") as file_test:\n",
        "    json.dump(preprocessed_test_texts, file_test)\n",
        "\n",
        "# Define DataLoader for the test dataset\n",
        "try:\n",
        "    padded_input_ids_test = pad_sequence([torch.tensor(tokens_test[:max_length] + [tokenizer_roberta.pad_token_id] * (max_length - len(tokens_test))) for tokens_test in preprocessed_test_texts], batch_first=True)\n",
        "except FileNotFoundError:\n",
        "    print(\"Preprocessed test dataset file not found.\")\n",
        "    exit()\n",
        "\n",
        "# Create a TensorDataset for the test dataset\n",
        "test_dataset = TensorDataset(padded_input_ids_test)\n",
        "\n",
        "# Define batch size for the test dataset\n",
        "batch_size_test = 32\n",
        "\n",
        "# Create DataLoader for the test dataset\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size_test)\n",
        "\n",
        "# Set model to evaluation mode\n",
        "roberta_model.eval()\n",
        "\n",
        "# Initialize lists to store predicted labels and actual labels\n",
        "predicted_labels = []\n",
        "actual_labels = []\n",
        "predictions = []  # Initialize the predictions list\n",
        "\n",
        "# Iterate over batches in the test dataset\n",
        "for batch_test in test_dataloader:\n",
        "    batch_test = tuple(t.to(device) for t in batch_test)\n",
        "    inputs_test = {\"input_ids\": batch_test[0]}\n",
        "    with torch.no_grad():\n",
        "        outputs_test = roberta_model(**inputs_test)\n",
        "    logits_test = outputs_test.logits\n",
        "    predicted_labels_test = torch.argmax(logits_test, dim=-1)\n",
        "    predictions.extend(predicted_labels_test.cpu().numpy())\n",
        "    # Add actual labels to the list\n",
        "    actual_labels.extend(batch_test[0].cpu().numpy())\n"
      ],
      "metadata": {
        "id": "J9fdXqrPJ0M9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "correct_predictions = sum(1 for pred, label in zip(predictions, actual_labels) if (pred == label).all())\n",
        "accuracy = correct_predictions / len(actual_labels)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V72K2ILDJ32Y",
        "outputId": "8ac43919-cf47-46a5-a034-ec4b20da6fcf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.011217948717948718\n"
          ]
        }
      ]
    }
  ]
}